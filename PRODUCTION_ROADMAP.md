# Radiology AI Assistant
# Complete Production Roadmap

**Document Version:** 1.0  
**Date:** January 2026  
**Project Status:** Development Complete - Ready for Production Deployment

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Current Project Status](#2-current-project-status)
3. [Production Deployment Steps](#3-production-deployment-steps)
4. [Infrastructure Setup](#4-infrastructure-setup)
5. [Security Hardening](#5-security-hardening)
6. [Database Migration](#6-database-migration)
7. [Model Considerations](#7-model-considerations)
8. [Testing Requirements](#8-testing-requirements)
9. [Monitoring & Observability](#9-monitoring--observability)
10. [Compliance & Regulatory](#10-compliance--regulatory)
11. [Maintenance Plan](#11-maintenance-plan)
12. [Cost Estimation](#12-cost-estimation)
13. [Timeline](#13-timeline)
14. [Checklist](#14-production-checklist)

---

## 1. Executive Summary

### What You Have Built

The **Radiology AI Assistant** is an AI-powered medical imaging analysis platform that:

- **Analyzes DICOM files** from X-ray, CT, and MRI scans
- **Detects pathologies** using deep learning models
- **Generates clinical recommendations** using GPT-4 and RAG
- **Provides secure access** via OAuth2/JWT authentication
- **Offers a web interface** for upload and result viewing

### Technology Stack

| Component | Technology |
|-----------|------------|
| Backend API | FastAPI (Python) |
| AI Models | TorchXRayVision, MONAI |
| RAG Pipeline | OpenAI GPT-4 + Pinecone |
| Authentication | OAuth2 + JWT |
| Frontend | HTML/CSS/JavaScript |
| Containerization | Docker + Docker Compose |
| Target Deployment | AWS (EC2 or ECS) |

### Current Capabilities

| Feature | Status | Production Ready |
|---------|--------|------------------|
| X-ray Analysis (18 pathologies) | ✅ Complete | Yes |
| CT Analysis (12 findings) | ✅ Complete | Needs fine-tuning |
| MRI Analysis (9 findings) | ✅ Complete | Needs fine-tuning |
| Clinical Recommendations (RAG) | ✅ Complete | Yes |
| User Authentication | ✅ Complete | Yes |
| API Endpoints | ✅ Complete | Yes |
| Web Frontend | ✅ Complete | Yes |
| Docker Deployment | ✅ Complete | Yes |

---

## 2. Current Project Status

### Completed Components

```
radio_assistance/
├── mainapp/
│   ├── auth.py                    # OAuth2/JWT authentication
│   ├── process_dicom_endpoint.py  # FastAPI endpoints
│   ├── the_nodes.py               # LangGraph workflow
│   ├── relevance_checker.py       # DICOM processing
│   ├── tensor_presenter.py        # X-ray model (TorchXRayVision)
│   ├── ct_mri_presenter.py        # CT/MRI 3D model (MONAI)
│   ├── rag_pipeline.py            # RAG with Pinecone + GPT-4
│   ├── knowledge_base.py          # X-ray clinical guidelines
│   ├── ct_mri_knowledge_base.py   # CT/MRI clinical guidelines
│   └── stateclass.py              # State definitions
├── config/
│   └── settings.py                # Configuration
└── ...

frontend/
├── index.html                     # Login page
├── dashboard.html                 # Dashboard
├── analysis.html                  # Results view
├── css/style.css                  # Styling
└── js/
    ├── api.js                     # API client
    └── app.js                     # App logic

Deployment Files:
├── Dockerfile                     # Container build
├── docker-compose.yml             # Multi-container setup
├── nginx.conf                     # Reverse proxy
├── requirements.txt               # Python dependencies
├── start.ps1                      # Windows startup
├── start.sh                       # Linux startup
└── aws-deployment/                # AWS guides
```

### What Works Right Now

1. **Upload DICOM files** via web interface or API
2. **Automatic modality detection** (X-ray, CT, MRI)
3. **AI analysis** with pathology detection
4. **Clinical recommendations** generated by GPT-4
5. **User authentication** with role-based access
6. **API documentation** at `/docs`

---

## 3. Production Deployment Steps

### Phase 1: Environment Preparation (Day 1)

#### Step 1.1: Create AWS Account (if not exists)
```
1. Go to aws.amazon.com
2. Create account with business email
3. Enable MFA on root account
4. Create IAM admin user (don't use root)
```

#### Step 1.2: Generate Production Secrets
```bash
# Generate JWT secret (run locally)
python -c "import secrets; print(secrets.token_urlsafe(64))"

# Result example:
# OiUdut1bM4sMdFz6TDex3umQqyAUj0U1gXNeRzfGIl9teobDfmC6Xp_lYgYw_geMpT0VieazBfVolJ4CK-ET6w
```

#### Step 1.3: Prepare API Keys
- **OpenAI API Key**: From platform.openai.com
- **Pinecone API Key**: From app.pinecone.io (free tier works)
- **JWT Secret Key**: Generated above

#### Step 1.4: Create .env File
```env
# Production Environment Variables
OPENAI_API_KEY=sk-proj-your-actual-key
PINECONE_API_KEY=your-pinecone-key
JWT_SECRET_KEY=your-generated-64-char-secret
CORS_ORIGINS=https://your-domain.com,http://your-elastic-ip
```

---

### Phase 2: AWS Infrastructure Setup (Day 1-2)

#### Step 2.1: Launch EC2 Instance

**Console Path:** AWS Console → EC2 → Launch Instance

**Recommended Settings:**
| Setting | Value | Reason |
|---------|-------|--------|
| Name | radiology-ai-prod | Identification |
| AMI | Ubuntu 22.04 LTS | Stable, well-supported |
| Instance Type | t3.large | 2 vCPU, 8GB RAM for AI models |
| Storage | 50GB gp3 | Space for uploads |
| Key Pair | Create new | For SSH access |

**Security Group Rules:**
| Type | Port | Source | Purpose |
|------|------|--------|---------|
| SSH | 22 | Your IP only | Admin access |
| HTTP | 80 | 0.0.0.0/0 | Web traffic |
| HTTPS | 443 | 0.0.0.0/0 | Secure web traffic |
| Custom TCP | 8000 | 0.0.0.0/0 | API (optional) |

#### Step 2.2: Allocate Elastic IP

```
1. EC2 Console → Elastic IPs → Allocate
2. Select the IP → Actions → Associate
3. Associate with your EC2 instance
4. Note the IP: XX.XX.XX.XX
```

**Why?** Elastic IP persists across instance restarts.

#### Step 2.3: Connect to Instance

```bash
# From your local machine
ssh -i "your-key.pem" ubuntu@XX.XX.XX.XX
```

#### Step 2.4: Install Docker

```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install Docker
sudo apt install -y docker.io docker-compose

# Start Docker
sudo systemctl enable docker
sudo systemctl start docker

# Add user to docker group
sudo usermod -aG docker ubuntu

# Logout and reconnect for group to take effect
exit
```

---

### Phase 3: Application Deployment (Day 2)

#### Step 3.1: Transfer Code to Server

**Option A: Git Clone (Recommended)**
```bash
ssh -i "your-key.pem" ubuntu@XX.XX.XX.XX
git clone https://github.com/YOUR_USERNAME/rass.git
cd rass
```

**Option B: SCP Upload**
```bash
# From local machine
scp -i "your-key.pem" -r ./rass ubuntu@XX.XX.XX.XX:~/
```

#### Step 3.2: Create Production .env

```bash
cd rass

cat > .env << 'EOF'
OPENAI_API_KEY=sk-proj-YOUR_OPENAI_KEY_HERE
PINECONE_API_KEY=YOUR_PINECONE_KEY_HERE
JWT_SECRET_KEY=YOUR_JWT_SECRET_HERE
CORS_ORIGINS=http://XX.XX.XX.XX
EOF
```

#### Step 3.3: Build and Run

```bash
# Build containers
docker-compose build

# Start in background
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f
```

#### Step 3.4: Verify Deployment

```bash
# Health check
curl http://localhost:8000/health

# Expected response:
# {"status":"healthy","version":"1.0.0"}
```

#### Step 3.5: Access Application

| Service | URL |
|---------|-----|
| Frontend | http://XX.XX.XX.XX |
| API | http://XX.XX.XX.XX:8000 |
| API Docs | http://XX.XX.XX.XX:8000/docs |

---

### Phase 4: HTTPS Setup (Day 2-3)

#### Option A: Self-Signed Certificate (Quick, shows warning)

```bash
# Generate certificate
sudo mkdir -p /etc/nginx/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /etc/nginx/ssl/nginx.key \
  -out /etc/nginx/ssl/nginx.crt \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=XX.XX.XX.XX"

# Update docker-compose to use SSL nginx config
# (use aws-deployment/nginx-ssl.conf)
```

#### Option B: AWS Application Load Balancer + ACM (Production)

```
1. Create Application Load Balancer
   - Internet-facing
   - Listeners: HTTP (80), HTTPS (443)
   
2. Request ACM Certificate
   - AWS Certificate Manager → Request
   - Domain: your-domain.com (or *.elb.amazonaws.com)
   - Validate via DNS
   
3. Configure Target Group
   - Target: Your EC2 instance
   - Port: 8000
   - Health check: /health
   
4. Configure HTTPS Listener
   - Select ACM certificate
   - Forward to target group
```

#### Option C: Let's Encrypt (Free, requires domain)

```bash
# Install certbot
sudo apt install certbot python3-certbot-nginx

# Get certificate (requires domain pointing to server)
sudo certbot --nginx -d your-domain.com
```

---

## 4. Infrastructure Setup

### Recommended Architecture

```
                    ┌─────────────────┐
                    │   Route 53      │
                    │  (DNS - later)  │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │      ALB        │
                    │ (Load Balancer) │
                    │   HTTPS:443     │
                    └────────┬────────┘
                             │
              ┌──────────────┴──────────────┐
              │                              │
     ┌────────▼────────┐           ┌────────▼────────┐
     │   EC2 (API)     │           │   EC2 (API)     │
     │  t3.large       │           │  t3.large       │
     │  Docker         │           │  Docker         │
     └────────┬────────┘           └────────┬────────┘
              │                              │
              └──────────────┬──────────────┘
                             │
                    ┌────────▼────────┐
                    │    RDS / S3     │
                    │  (Future DB)    │
                    └─────────────────┘
```

### Minimum Viable Production (MVP)

For immediate deployment, use single EC2:

```
     Internet
        │
        ▼
  ┌─────────────┐
  │ Elastic IP  │
  │ XX.XX.XX.XX │
  └──────┬──────┘
         │
  ┌──────▼──────┐
  │    EC2      │
  │  t3.large   │
  │             │
  │ ┌─────────┐ │
  │ │  Nginx  │ │ ◄── Port 80/443
  │ │ (proxy) │ │
  │ └────┬────┘ │
  │      │      │
  │ ┌────▼────┐ │
  │ │ FastAPI │ │ ◄── Port 8000
  │ │   API   │ │
  │ └─────────┘ │
  └─────────────┘
```

---

## 5. Security Hardening

### 5.1 Change Default Passwords

**CRITICAL: Before going live, update default credentials in `auth.py`:**

```python
# Current (INSECURE for production):
USERS_DB = {
    "admin": {"password": "admin123", ...},
    "radiologist": {"password": "rad123", ...},
}

# Production: Either:
# 1. Remove default users entirely
# 2. Change to strong passwords
# 3. Implement proper user registration with email verification
```

### 5.2 Environment Variables

Never commit `.env` to git. Use:
- AWS Secrets Manager
- Environment variables in ECS task definition
- EC2 Systems Manager Parameter Store

### 5.3 Network Security

```bash
# Update Security Group to restrict:
# - SSH (22): Only from your office IP
# - Remove port 8000 public access (use nginx proxy)
```

### 5.4 JWT Configuration

```python
# In auth.py - ensure production settings:
ACCESS_TOKEN_EXPIRE_MINUTES = 30  # Reduce from 60
SECRET_KEY = os.getenv("JWT_SECRET_KEY")  # Never hardcode
```

### 5.5 CORS Configuration

```python
# In process_dicom_endpoint.py
CORS_ORIGINS = os.getenv("CORS_ORIGINS").split(",")
# Set to your actual domain, not "*"
```

### 5.6 Rate Limiting (Add)

```python
# Install: pip install slowapi
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/api/analyze")
@limiter.limit("10/minute")  # 10 requests per minute per IP
async def analyze_dicom(...):
    ...
```

---

## 6. Database Migration

### Current State: In-Memory
Currently, users are stored in `USERS_DB` dict in `auth.py`. This:
- ❌ Loses data on restart
- ❌ Doesn't scale
- ❌ Not suitable for production

### Production: PostgreSQL

#### Step 6.1: Create RDS Instance

```
AWS Console → RDS → Create Database
- Engine: PostgreSQL 14+
- Template: Free tier (for MVP) or Production
- Instance: db.t3.micro (MVP) or db.t3.small
- Storage: 20GB gp2
- VPC: Same as EC2
- Public access: No (access via EC2 only)
```

#### Step 6.2: Install SQLAlchemy

```bash
pip install sqlalchemy psycopg2-binary alembic
```

#### Step 6.3: Create Database Models

```python
# models.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True)
    username = Column(String(50), unique=True, index=True)
    email = Column(String(100), unique=True)
    hashed_password = Column(String(255))
    full_name = Column(String(100))
    role = Column(String(20), default="user")
    disabled = Column(Boolean, default=False)
    created_at = Column(DateTime, server_default="now()")

class Study(Base):
    __tablename__ = "studies"
    
    id = Column(Integer, primary_key=True)
    study_id = Column(String(36), unique=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"))
    modality = Column(String(10))
    status = Column(String(20))
    findings = Column(JSON)
    recommendations = Column(Text)
    created_at = Column(DateTime, server_default="now()")
```

#### Step 6.4: Migrate User Authentication

```python
# auth.py updates
from sqlalchemy.orm import Session
from .database import get_db
from .models import User

def get_user(db: Session, username: str):
    return db.query(User).filter(User.username == username).first()

def create_user(db: Session, user_data: UserCreate):
    hashed_password = get_password_hash(user_data.password)
    db_user = User(
        username=user_data.username,
        email=user_data.email,
        hashed_password=hashed_password,
        full_name=user_data.full_name,
        role=user_data.role
    )
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user
```

---

## 7. Model Considerations

### 7.1 X-Ray Model: Production Ready ✅

**Model:** TorchXRayVision `densenet121-res224-all`

- Trained on 7 major datasets (ChestX-ray14, CheXpert, MIMIC-CXR, etc.)
- Published, peer-reviewed
- 18 pathology classes
- **Ready for production use**

### 7.2 CT/MRI Model: Needs Fine-Tuning ⚠️

**Model:** MONAI DenseNet-121 3D

**Current State:**
- Randomly initialized weights
- Will produce random predictions
- **NOT production ready**

**To Make Production Ready:**

#### Option A: Fine-tune on Your Data
```python
# Requires labeled CT/MRI dataset
# Example: LIDC-IDRI (lung CT), BraTS (brain MRI)

from monai.networks.nets import DenseNet121
from monai.losses import DiceLoss
from monai.optimizers import Novograd

model = DenseNet121(spatial_dims=3, in_channels=1, out_channels=12)
optimizer = Novograd(model.parameters(), lr=1e-3)
loss_fn = DiceLoss()

# Training loop...
for epoch in range(100):
    for batch in train_loader:
        ...
```

#### Option B: Use Pre-trained MONAI Bundles
```bash
# Download pre-trained model from MONAI Model Zoo
python -m monai.bundle download --name lung_nodule_ct_detection

# Available bundles:
# - lung_nodule_ct_detection
# - spleen_ct_segmentation
# - brain_tumor_mri_segmentation
```

#### Option C: Disable CT/MRI Analysis (Honest Approach)
```python
# In ct_mri_presenter.py
def analyze_volume(self, ...):
    return {
        "predictions": {
            "positive_findings": [],
            "top_predictions": [],
            "note": "CT/MRI analysis requires fine-tuned model. Contact administrator."
        }
    }
```

### 7.3 Model Performance Monitoring

Track these metrics in production:
- Inference latency (target: <5s for X-ray, <30s for CT)
- GPU memory usage
- Prediction confidence distributions
- False positive/negative feedback

---

## 8. Testing Requirements

### 8.1 Unit Tests

```python
# tests/test_auth.py
def test_login_success():
    response = client.post("/api/auth/token", data={
        "username": "admin", "password": "admin123"
    })
    assert response.status_code == 200
    assert "access_token" in response.json()

def test_login_failure():
    response = client.post("/api/auth/token", data={
        "username": "admin", "password": "wrong"
    })
    assert response.status_code == 401
```

### 8.2 Integration Tests

```python
# tests/test_analysis.py
def test_xray_analysis():
    # Upload test DICOM
    with open("tests/fixtures/test_xray.dcm", "rb") as f:
        response = client.post(
            "/api/analyze",
            files={"files": f},
            headers={"Authorization": f"Bearer {token}"}
        )
    assert response.status_code == 200
    assert response.json()["status"] == "completed"
    assert "findings" in response.json()
```

### 8.3 Load Testing

```bash
# Using locust
pip install locust

# locustfile.py
from locust import HttpUser, task

class RadiologyUser(HttpUser):
    @task
    def health_check(self):
        self.client.get("/health")
    
    @task
    def analyze(self):
        with open("test.dcm", "rb") as f:
            self.client.post("/api/analyze", files={"files": f})

# Run: locust -f locustfile.py --host=http://localhost:8000
```

### 8.4 Clinical Validation

**Before clinical use, validate with radiologists:**
1. Collect 100+ annotated test cases per modality
2. Calculate sensitivity/specificity per pathology
3. Compare AI findings vs radiologist findings
4. Document discrepancies
5. Obtain approval from medical director

---

## 9. Monitoring & Observability

### 9.1 CloudWatch Logs

```bash
# docker-compose.yml - add logging
services:
  api:
    logging:
      driver: awslogs
      options:
        awslogs-group: /ecs/radiology-ai
        awslogs-region: us-east-1
        awslogs-stream-prefix: api
```

### 9.2 Health Checks

```python
# Enhanced health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "1.0.0",
        "models": {
            "xray": "loaded" if _xray_presenter._model else "not_loaded",
            "ct_mri": "loaded" if _ct_mri_presenter._model else "not_loaded"
        },
        "rag": "connected" if get_rag_pipeline() else "disconnected",
        "timestamp": datetime.utcnow().isoformat()
    }
```

### 9.3 Error Tracking (Sentry)

```bash
pip install sentry-sdk[fastapi]
```

```python
import sentry_sdk
sentry_sdk.init(
    dsn="https://xxx@sentry.io/xxx",
    traces_sample_rate=0.1
)
```

### 9.4 Metrics Dashboard

Key metrics to track:
- Request latency (P50, P95, P99)
- Error rate (4xx, 5xx)
- Active users
- Studies analyzed per day
- Model inference time
- API token usage (OpenAI)

---

## 10. Compliance & Regulatory

### 10.1 HIPAA Compliance (US Healthcare)

| Requirement | Status | Action Needed |
|-------------|--------|---------------|
| PHI Encryption at Rest | ⚠️ | Enable EBS encryption |
| PHI Encryption in Transit | ⚠️ | Enable HTTPS |
| Access Controls | ✅ | Role-based auth implemented |
| Audit Logging | ⚠️ | Add detailed access logs |
| BAA with AWS | ❌ | Sign Business Associate Agreement |
| Data Backup | ⚠️ | Configure S3 backups |

### 10.2 FDA Considerations

**This software may be considered a medical device if:**
- Used for diagnosis or treatment decisions
- Marketed with clinical claims

**If FDA regulated:**
1. 510(k) clearance required
2. Quality Management System (ISO 13485)
3. Risk analysis documentation
4. Clinical validation studies

**Current Recommendation:**
- Use as "clinical decision support" only
- Always require radiologist confirmation
- Include prominent disclaimers

### 10.3 Disclaimers (Required)

Add to all outputs:
```
DISCLAIMER: This analysis was generated by AI and is intended for 
clinical decision support only. All findings must be verified by a 
qualified radiologist. This software is not FDA-cleared for diagnostic use.
```

---

## 11. Maintenance Plan

### 11.1 Daily Tasks (Automated)

- [ ] Health check monitoring
- [ ] Log rotation
- [ ] Backup verification

### 11.2 Weekly Tasks

- [ ] Review error logs
- [ ] Check disk space
- [ ] Review API usage/costs
- [ ] Security updates check

### 11.3 Monthly Tasks

- [ ] Performance review
- [ ] Cost optimization
- [ ] User feedback analysis
- [ ] Model performance review
- [ ] Dependency updates

### 11.4 Quarterly Tasks

- [ ] Security audit
- [ ] Disaster recovery test
- [ ] Clinical validation review
- [ ] Feature roadmap review

---

## 12. Cost Estimation

### AWS Costs (Monthly)

| Resource | Specification | Cost |
|----------|---------------|------|
| EC2 (t3.large) | 2 vCPU, 8GB, 24/7 | ~$60 |
| Elastic IP | Static IP | $3.60 |
| EBS Storage | 50GB gp3 | ~$4 |
| Data Transfer | ~50GB/month | ~$5 |
| **Subtotal AWS** | | **~$73** |

### External Services (Monthly)

| Service | Usage | Cost |
|---------|-------|------|
| OpenAI API | ~1000 analyses | ~$50-100 |
| Pinecone | Free tier | $0 |
| **Subtotal External** | | **~$50-100** |

### Total Monthly Cost: **~$125-175**

### Scaling Costs

| Scale | Users | Studies/Month | Est. Cost |
|-------|-------|---------------|-----------|
| MVP | 10 | 100 | $125 |
| Small | 50 | 500 | $300 |
| Medium | 200 | 2000 | $800 |
| Large | 1000 | 10000 | $3000+ |

---

## 13. Timeline

### Week 1: Core Deployment

| Day | Task | Owner |
|-----|------|-------|
| 1 | AWS account setup, EC2 launch | Admin |
| 1 | Security group, Elastic IP | Admin |
| 2 | Docker deployment | DevOps |
| 2 | Initial testing | QA |
| 3 | HTTPS setup | DevOps |
| 3 | Production .env | Admin |
| 4 | User acceptance testing | Team |
| 5 | Go-live (soft launch) | Team |

### Week 2: Hardening

| Day | Task | Owner |
|-----|------|-------|
| 1-2 | Security audit | Security |
| 3 | Monitoring setup | DevOps |
| 4 | Documentation | Team |
| 5 | Training | Team |

### Week 3-4: Stabilization

- Bug fixes
- Performance optimization
- User feedback collection
- Feature prioritization

### Month 2+: Enhancements

- Database migration (PostgreSQL)
- CT/MRI model fine-tuning
- Mobile-responsive frontend
- Advanced reporting
- Integration APIs

---

## 14. Production Checklist

### Pre-Deployment

- [ ] API keys secured (not in code)
- [ ] JWT secret generated (64+ chars)
- [ ] Default passwords changed
- [ ] CORS configured for production domain
- [ ] .env file created on server
- [ ] Docker images built
- [ ] Health endpoint tested

### Infrastructure

- [ ] EC2 instance launched
- [ ] Security groups configured
- [ ] Elastic IP allocated
- [ ] SSH key secured
- [ ] Docker installed
- [ ] Firewall rules verified

### Security

- [ ] HTTPS enabled
- [ ] No sensitive data in logs
- [ ] Rate limiting enabled
- [ ] Input validation active
- [ ] Error messages sanitized
- [ ] Access logs enabled

### Monitoring

- [ ] Health checks configured
- [ ] Error alerting setup
- [ ] Log aggregation working
- [ ] Backup strategy defined

### Documentation

- [ ] API documentation current
- [ ] User guide available
- [ ] Admin guide available
- [ ] Runbook for common issues

### Legal/Compliance

- [ ] Disclaimers added
- [ ] Terms of service drafted
- [ ] Privacy policy drafted
- [ ] Data retention policy defined

---

## Quick Start Commands

```bash
# SSH to server
ssh -i "your-key.pem" ubuntu@XX.XX.XX.XX

# View logs
docker-compose logs -f

# Restart services
docker-compose restart

# Stop services
docker-compose down

# Start services
docker-compose up -d

# Check health
curl http://localhost:8000/health

# Update application
git pull
docker-compose build
docker-compose up -d
```

---

## Support Contacts

| Role | Contact | Responsibility |
|------|---------|----------------|
| Project Owner | [Your Name] | Business decisions |
| DevOps Lead | [Name] | Infrastructure |
| ML Engineer | [Name] | Model issues |
| Security Lead | [Name] | Security concerns |

---

**Document End**

*This document should be updated as the project evolves.*

